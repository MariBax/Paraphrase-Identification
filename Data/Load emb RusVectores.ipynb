{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run parse_xml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>8159</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>8158</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>273</td>\n",
       "      <td>8167</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id id_1  id_2                                             text_1  \\\n",
       "0  1  201  8159  Полицейским разрешат стрелять на поражение по ...   \n",
       "1  2  202  8158  Право полицейских на проникновение в жилище ре...   \n",
       "2  3  273  8167  Президент Египта ввел чрезвычайное положение в...   \n",
       "\n",
       "                                              text_2   jaccard class  \n",
       "0  Полиции могут разрешить стрелять по хулиганам ...      0.65     0  \n",
       "1  Правила внесудебного проникновения полицейских...       0.5     0  \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...  0.611429     0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df = parse_XML(\"paraphrases_mod.xml\", [\"id\", \"id_1\", \"id_2\", \"text_1\", \"text_2\", \"jaccard\", \"class\"])\n",
    "xml_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4270, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df_fltr = xml_df.loc[xml_df['class'].isin(['-1', '1'])]\n",
    "xml_df_fltr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(xml_df_fltr.shape[0]):\n",
    "    if xml_df_fltr.iloc[ind]['class'] == '-1':\n",
    "        xml_df_fltr.iloc[ind]['class'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gensim \n",
    "with zipfile.ZipFile('ru_news_model.zip', 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model_w2v_rv = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n"
     ]
    }
   ],
   "source": [
    "import text_processing_rv\n",
    "%run text_processing_rv.py\n",
    "import os\n",
    "\n",
    "# URL of the UDPipe model\n",
    "udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "if not os.path.isfile(udpipe_filename):\n",
    "    print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "    wget.download(udpipe_model_url)\n",
    "\n",
    "print('\\nLoading the model...', file=sys.stderr)\n",
    "model = Model.load(udpipe_filename)\n",
    "process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['владимир::путин_PROPN ',\n",
       " 'принимать_VERB',\n",
       " 'указ_NOUN',\n",
       " 'о_ADP',\n",
       " 'новость_NOUN']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(process_pipeline, text=unify_sym('Владимир Путин принял указ о новостях.'.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_rv_names = dict()\n",
    "with open(\"mapping_rv.txt\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split(' ')\n",
    "#         print(values)\n",
    "        mapping_rv_names[values[0]] = values[1].strip('\\n')\n",
    "        \n",
    "# print(mapping_rv_names)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words = get_stop_words('russian')\n",
    "stop_words.append(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', \n",
    "                   'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', \n",
    "                   'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', \n",
    "                   'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', \n",
    "                   'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', \n",
    "                   'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', \n",
    "                   'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', \n",
    "                   'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', \n",
    "                   'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', \n",
    "                   'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "udp_sentence_one = []\n",
    "udp_sentence_two = []\n",
    "udp_set_words = set([])\n",
    "\n",
    "for sent in xml_df_fltr['text_1']:   \n",
    "    sent_coded = [word.rstrip() for word in process(process_pipeline, text=unify_sym(sent.strip()))] # trim strange spases in PROPN\n",
    "    sent_coded_upd = []\n",
    "    for word in sent_coded:\n",
    "        if word in mapping_rv_names.keys():\n",
    "            sent_coded_upd.append(mapping_rv_names[word])\n",
    "        elif word[-4:] not in ['_ADP', '_DET', '_SYM'] and word[-5:] not in ['_PART', '_PRON', \n",
    "                            'CCONJ', 'SCONJ'] and word[:word.index('_')] not in stop_words:\n",
    "            sent_coded_upd.append(word.lstrip('-')) # были слова, начинавшиеся с '-'\n",
    "    udp_sentence_one.append(sent_coded_upd) \n",
    "    for word in sent_coded_upd:\n",
    "        udp_set_words.add(word)\n",
    "        \n",
    "for sent in xml_df_fltr['text_2']:   \n",
    "    sent_coded = [word.rstrip() for word in process(process_pipeline, text=unify_sym(sent.strip()))] # trim strange spases in PROPN\n",
    "    sent_coded_upd = []\n",
    "    for word in sent_coded:\n",
    "        if word in mapping_rv_names.keys():\n",
    "            sent_coded_upd.append(mapping_rv_names[word])\n",
    "        elif word[-4:] not in ['_ADP', '_DET', '_SYM'] and word[-5:] not in ['_PART', '_PRON', \n",
    "                            'CCONJ', 'SCONJ'] and word[:word.index('_')] not in stop_words:\n",
    "            sent_coded_upd.append(word.lstrip('-'))\n",
    "    udp_sentence_two.append(sent_coded_upd) \n",
    "    for word in sent_coded_upd:\n",
    "        udp_set_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the file in write binary mode\n",
    "with open('sentence_one_rusvec_processed_2.txt', 'wb') as F:\n",
    "    # Dump the list to file\n",
    "    pickle.dump(udp_sentence_one, F)\n",
    "\n",
    "# Open the file in write binary mode\n",
    "with open('sentence_two_rusvec_processed_2.txt', 'wb') as F:\n",
    "    # Dump the list to file\n",
    "    pickle.dump(udp_sentence_two, F)\n",
    "    \n",
    "# Open the file in write binary mode\n",
    "with open('unq_words_rusvec_processed_2.txt', 'wb') as F:\n",
    "    # Dump the list to file\n",
    "    pickle.dump(udp_set_words, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пересечений  8212\n",
      "Количество слов, котрых нет в модели  437\n"
     ]
    }
   ],
   "source": [
    "intersect_mydata_rusvect = set.intersection(set(model_w2v_rv.vocab.keys()), udp_set_words)\n",
    "print(\"Количество пересечений \", len(intersect_mydata_rusvect))\n",
    "\n",
    "difference = udp_set_words - set(model_w2v_rv.vocab.keys())\n",
    "print(\"Количество слов, которых нет в модели \", len(difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пар предложений, выпадающие изза отсутствия слова в модели  833\n",
      "Общее количество пар  4270\n"
     ]
    }
   ],
   "source": [
    "global_count = 0\n",
    "ids_to_take = [i for i in range(len(udp_sentence_one))]\n",
    "\n",
    "for sent_1, sent_2, i in zip(udp_sentence_one, udp_sentence_two, range(len(udp_sentence_one))):\n",
    "    count = 0\n",
    "    for word in sent_1:\n",
    "        if word in difference:\n",
    "            count += 1\n",
    "    for word in sent_2:\n",
    "        if word in difference:\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "#         print(sent_1, sent_2)\n",
    "        global_count += 1\n",
    "        ids_to_take.remove(i)\n",
    "        \n",
    "print(\"Количество пар предложений, выпадающие из-за отсутствия слова в модели \", global_count)\n",
    "print(\"Общее количество пар \", len(udp_sentence_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_df_fltr_2 = xml_df_fltr.loc[xml_df_fltr.index[ids_to_take], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3437, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df_fltr_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    2011\n",
       "1    1426\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df_fltr_2.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df_fltr_2_ones = xml_df_fltr_2.loc[xml_df_fltr_2['class'].isin(['1'])]\n",
    "xml_df_fltr_2_ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2011, 7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df_fltr_2_zeros = xml_df_fltr_2.loc[xml_df_fltr_2['class'].isin(['0'])]\n",
    "xml_df_fltr_2_zeros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_df_fltr_2_zeros_ = xml_df_fltr_2_zeros[0:1426]\n",
    "xml_df_fltr_2_zeros_.index = xml_df_fltr_2_ones.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2852, 7)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df_fltr_3 = pd.concat([xml_df_fltr_2_ones, xml_df_fltr_2_zeros_], axis= 0)\n",
    "xml_df_fltr_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     xml_df_fltr_3, xml_df_fltr_3, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    454\n",
       "1    488\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.groupby(['class']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    972\n",
       "1    938\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.groupby(['class']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['class', 'text_1', 'text_2', 'id']].to_csv('test_PP.tsv', sep = '\\t', header = None)\n",
    "X_train[['class', 'text_1', 'text_2', 'id']].to_csv('train_PP.tsv', sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999948601688"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(j**2 for j in model_w2v_rv['цб_PROPN'] / sum([i**2 for i in model_w2v_rv['цб_PROPN']])**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wordvec_rv_3.txt\", \"w\") as file:\n",
    "    for word in intersect_mydata_rusvect:\n",
    "        vect_norm = model_w2v_rv[word] / ((sum(i**2 for i in model_w2v_rv[word]))**(1/2))\n",
    "        line_list = [word] + vect_norm.tolist()\n",
    "        file.write(' '.join(str(item) for item in line_list) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
