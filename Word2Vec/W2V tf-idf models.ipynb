{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%run parse_xml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>8159</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>8158</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>273</td>\n",
       "      <td>8167</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id id_1  id_2                                             text_1  \\\n",
       "0  1  201  8159  Полицейским разрешат стрелять на поражение по ...   \n",
       "1  2  202  8158  Право полицейских на проникновение в жилище ре...   \n",
       "2  3  273  8167  Президент Египта ввел чрезвычайное положение в...   \n",
       "\n",
       "                                              text_2   jaccard class  \n",
       "0  Полиции могут разрешить стрелять по хулиганам ...      0.65     0  \n",
       "1  Правила внесудебного проникновения полицейских...       0.5     0  \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...  0.611429     0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df = parse_XML(\"paraphrases_mod.xml\", [\"id\", \"id_1\", \"id_2\", \"text_1\", \"text_2\", \"jaccard\", \"class\"])\n",
    "xml_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4270, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df_fltr = xml_df.loc[xml_df['class'].isin(['-1', '1'])]\n",
    "xml_df_fltr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "-1    2582\n",
       "1     1688\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_df_fltr.groupby(['class']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gensim \n",
    "import pymorphy2\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "list_punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_one = [] \n",
    "sentence_two = list(list_punctuation) + ['\"', '’', \"''\", '``', '`', '—', '«', '»']\n",
    "\n",
    "# iterate through each sentence in dataset \n",
    "for sent_1, sent_2 in zip(xml_df['text_1'], xml_df['text_2']): \n",
    "    temp_sent_1 = []\n",
    "    temp_sent_2 = []\n",
    "\n",
    "     #tokenize the sentence into words \n",
    "    for word in word_tokenize(sent_1):\n",
    "        if word not in list_punctuation:\n",
    "            temp_sent_1.append(morph.parse(word.lower())[0].normal_form)  \n",
    "    for word in word_tokenize(sent_2): \n",
    "        if word not in list_punctuation:\n",
    "            temp_sent_2.append(morph.parse(word.lower())[0].normal_form) \n",
    "\n",
    "    sentence_one.append(temp_sent_1) \n",
    "    sentence_two.append(temp_sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_w2v = sentence_one + sentence_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3028437, 3600240)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create CBOW model \n",
    "model_cbow = gensim.models.Word2Vec(data_for_w2v, min_count = 1, \n",
    "size = 300, window = 6, negative=20)\n",
    "model_cbow.train(data_for_w2v, total_examples=len(data_for_w2v), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('россия', 0.5653359889984131),\n",
       " ('норвегия', 0.5295307040214539),\n",
       " ('малайзия', 0.5271217823028564),\n",
       " ('125', 0.4573618471622467),\n",
       " ('монетарный', 0.43756282329559326),\n",
       " ('донбасс', 0.4372336268424988),\n",
       " ('ес', 0.43691155314445496),\n",
       " ('тариф', 0.4327203929424286),\n",
       " ('срочно', 0.4278368055820465),\n",
       " ('инопланетянин', 0.4275597333908081)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow.wv.most_similar(positive=[\"рф\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3028805, 3600240)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Skip Gram model \n",
    "model_sg = gensim.models.Word2Vec(data_for_w2v, min_count = 1, size = 300, \n",
    "window = 6, sg = 1) \n",
    "model_sg.train(data_for_w2v, total_examples=len(data_for_w2v), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сборная', 0.36934131383895874),\n",
       " ('липницкий', 0.35827547311782837),\n",
       " ('свежеть', 0.35358569025993347),\n",
       " ('миронов', 0.3527170419692993),\n",
       " ('недельный', 0.35179877281188965),\n",
       " ('острава', 0.34980159997940063),\n",
       " ('один-десять', 0.34942200779914856),\n",
       " ('полк', 0.3456084728240967),\n",
       " ('чм', 0.33804041147232056),\n",
       " ('фигуристка', 0.333640456199646)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar(positive=[\"россия\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr_sentence_one = [] \n",
    "fltr_sentence_two = []\n",
    "list(list_punctuation) + ['\"', '’', \"''\", '``', '`', '—', '«', '»']\n",
    "\n",
    "# iterate through each sentence in dataset \n",
    "for sent_1, sent_2 in zip(xml_df_fltr['text_1'], xml_df_fltr['text_2']): \n",
    "    temp_sent_1 = []\n",
    "    temp_sent_2 = []\n",
    "\n",
    "     #tokenize the sentence into words \n",
    "    for word in word_tokenize(sent_1):\n",
    "        if word not in list_punctuation:\n",
    "            temp_sent_1.append(morph.parse(word.lower())[0].normal_form)  \n",
    "    for word in word_tokenize(sent_2): \n",
    "        if word not in list_punctuation:\n",
    "            temp_sent_2.append(morph.parse(word.lower())[0].normal_form) \n",
    "\n",
    "    fltr_sentence_one.append(temp_sent_1) \n",
    "    fltr_sentence_two.append(temp_sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run tf-idf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_sentence_one = compute_tfidf(fltr_sentence_one)\n",
    "tf_idf_sentence_two = compute_tfidf(fltr_sentence_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sentence_one = np.zeros((len(fltr_sentence_one), 300))\n",
    "vec_sentence_two = np.zeros((len(fltr_sentence_one), 300))\n",
    "\n",
    "for sent_1, sent_2, tfidf_one, tfidf_two, row_id in zip(fltr_sentence_one, fltr_sentence_two, tf_idf_sentence_one, tf_idf_sentence_two, range(len(fltr_sentence_one))): \n",
    "    \n",
    "    vec_sentence_one[row_id] = sum(tfidf_one[word] * model_cbow[word] / sqrt(sum(i**2 for i in model_cbow[word])) for word in sent_1)\n",
    "    vec_sentence_two[row_id] = sum(tfidf_two[word] * model_cbow[word] / sqrt(sum(i**2 for i in model_cbow[word])) for word in sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros(xml_df_fltr.shape[0])\n",
    "\n",
    "for value, ind in zip(xml_df_fltr['class'], range(xml_df_fltr.shape[0])):\n",
    "    if value != '-1':\n",
    "        y[ind] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4270, 600)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cbow = np.hstack((vec_sentence_one, vec_sentence_two))\n",
    "X_cbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariya.bakhanova\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of oversampled data is  3690\n",
      "Proportion of no subscription data in oversampled data is  0.5\n",
      "Proportion of subscription data in oversampled data is  -0.4997289972899729\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "os = SMOTE(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cbow, y, test_size=0.3, random_state=0)\n",
    "os_data_X, os_data_y = os.fit_sample(X_train, y_train)\n",
    "\n",
    "print(\"Length of oversampled data is \", len(os_data_X))\n",
    "print(\"Proportion of no subscription data in oversampled data is \", np.sum(os_data_y) / len(os_data_X))\n",
    "print(\"Proportion of subscription data in oversampled data is \", (1 - np.sum(os_data_y)) / len(os_data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(os_data_X, os_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.66\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.67      0.70       737\n",
      "         1.0       0.59      0.65      0.62       544\n",
      "\n",
      "    accuracy                           0.66      1281\n",
      "   macro avg       0.66      0.66      0.66      1281\n",
      "weighted avg       0.67      0.66      0.66      1281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sentence_one_sg = np.zeros((len(fltr_sentence_one), 300))\n",
    "vec_sentence_two_sg = np.zeros((len(fltr_sentence_one), 300))\n",
    "\n",
    "for sent_1, sent_2, tfidf_one, tfidf_two, row_id in zip(fltr_sentence_one, fltr_sentence_two, tf_idf_sentence_one, tf_idf_sentence_two, range(len(fltr_sentence_one))): \n",
    "    \n",
    "    vec_sentence_one_sg[row_id] = sum(tfidf_one[word] * model_sg[word] / sqrt(sum(i**2 for i in model_sg[word])) for word in sent_1)\n",
    "    vec_sentence_two_sg[row_id] = sum(tfidf_two[word] * model_sg[word] / sqrt(sum(i**2 for i in model_sg[word])) for word in sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4270, 600)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sg = np.hstack((vec_sentence_one_sg, vec_sentence_two_sg))\n",
    "X_sg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of oversampled data is  3690\n",
      "Proportion of no subscription data in oversampled data is  0.5\n",
      "Proportion of subscription data in oversampled data is  -0.4997289972899729\n"
     ]
    }
   ],
   "source": [
    "os = SMOTE(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sg, y, test_size=0.3, random_state=0)\n",
    "os_data_X, os_data_y = os.fit_sample(X_train, y_train)\n",
    "\n",
    "print(\"Length of oversampled data is \",len(os_data_X))\n",
    "print(\"Proportion of no subscription data in oversampled data is \", np.sum(os_data_y) / len(os_data_X))\n",
    "print(\"Proportion of subscription data in oversampled data is \", (1 - np.sum(os_data_y)) / len(os_data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(os_data_X, os_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.67\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.69      0.70       737\n",
      "         1.0       0.60      0.63      0.62       544\n",
      "\n",
      "    accuracy                           0.67      1281\n",
      "   macro avg       0.66      0.66      0.66      1281\n",
      "weighted avg       0.67      0.67      0.67      1281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('ru_news_model.zip', 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model_w2v_rv = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)\n",
    "\n",
    "# If you don't have this zip file on your computer you can use the following code:\n",
    "    \n",
    "# import zipfile\n",
    "# model_url = 'http://vectors.nlpl.eu/repository/11/184.zip'\n",
    "# m = wget.download(model_url)\n",
    "# model_file = model_url.split('/')[-1]\n",
    "# with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "#     stream = archive.open('model.bin')\n",
    "#     model_w2v_rv = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UDPipe model not found. Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 40616122 / 40616122"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n"
     ]
    }
   ],
   "source": [
    "import text_processing_rv\n",
    "%run text_processing_rv.py\n",
    "import os\n",
    "\n",
    "# URL of the UDPipe model\n",
    "udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "if not os.path.isfile(udpipe_filename):\n",
    "    print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "    wget.download(udpipe_model_url)\n",
    "\n",
    "print('\\nLoading the model...', file=sys.stderr)\n",
    "model = Model.load(udpipe_filename)\n",
    "process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['владимир::путин_PROPN ',\n",
       " 'принимать_VERB',\n",
       " 'указ_NOUN',\n",
       " 'о_ADP',\n",
       " 'новость_NOUN']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(process_pipeline, text=unify_sym('Владимир Путин принял указ о новостях.'.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_rv_names = dict()\n",
    "with open(\"mapping_rv.txt\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split(' ')\n",
    "#         print(values)\n",
    "        mapping_rv_names[values[0]] = values[1].strip('\\n')\n",
    "        \n",
    "# print(mapping_rv_names)     \n",
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words = get_stop_words('russian')\n",
    "stop_words.append(['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', \n",
    "                   'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', \n",
    "                   'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', \n",
    "                   'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', \n",
    "                   'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', \n",
    "                   'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', \n",
    "                   'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', \n",
    "                   'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', \n",
    "                   'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', \n",
    "                   'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "udp_sentence_one = []\n",
    "udp_sentence_two = []\n",
    "udp_set_words = set([])\n",
    "\n",
    "for sent in xml_df_fltr['text_1']:   \n",
    "    sent_coded = [word.rstrip() for word in process(process_pipeline, text=unify_sym(sent.strip()))] # trim strange spases in PROPN\n",
    "    sent_coded_upd = []\n",
    "    for word in sent_coded:\n",
    "        if word in mapping_rv_names.keys():\n",
    "            sent_coded_upd.append(mapping_rv_names[word])\n",
    "        elif word[-4:] not in ['_ADP', '_DET', '_SYM'] and word[-5:] not in ['_PART', '_PRON', \n",
    "                            'CCONJ', 'SCONJ'] and word[:word.index('_')] not in stop_words:\n",
    "            sent_coded_upd.append(word.lstrip('-')) # были слова, начинавшиеся с '-'\n",
    "    udp_sentence_one.append(sent_coded_upd) \n",
    "    for word in sent_coded_upd:\n",
    "        udp_set_words.add(word)\n",
    "        \n",
    "for sent in xml_df_fltr['text_2']:   \n",
    "    sent_coded = [word.rstrip() for word in process(process_pipeline, text=unify_sym(sent.strip()))] # trim strange spases in PROPN\n",
    "    sent_coded_upd = []\n",
    "    for word in sent_coded:\n",
    "        if word in mapping_rv_names.keys():\n",
    "            sent_coded_upd.append(mapping_rv_names[word])\n",
    "        elif word[-4:] not in ['_ADP', '_DET', '_SYM'] and word[-5:] not in ['_PART', '_PRON', \n",
    "                            'CCONJ', 'SCONJ'] and word[:word.index('_')] not in stop_words:\n",
    "            sent_coded_upd.append(word.lstrip('-'))\n",
    "    udp_sentence_two.append(sent_coded_upd) \n",
    "    for word in sent_coded_upd:\n",
    "        udp_set_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пересечений  8212\n",
      "Количество слов, котрых нет в модели  437\n"
     ]
    }
   ],
   "source": [
    "intersect_mydata_rusvect = set.intersection(set(model_w2v_rv.vocab.keys()), udp_set_words)\n",
    "print(\"Количество пересечений \", len(intersect_mydata_rusvect))\n",
    "\n",
    "difference = udp_set_words - set(model_w2v_rv.vocab.keys())\n",
    "print(\"Количество слов, котрых нет в модели \", len(difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пар предложений, выпадающих из-за отсутствия слова в модели  833\n",
      "Общее количество пар  4270\n"
     ]
    }
   ],
   "source": [
    "global_count = 0\n",
    "ids_to_delete = []\n",
    "\n",
    "udp_sentence_one_for_mod = []\n",
    "udp_sentence_two_for_mod = []\n",
    "\n",
    "ids_to_take = [i for i in range(len(udp_sentence_one))]\n",
    "i = 0\n",
    "\n",
    "for sent_1, sent_2 in zip(udp_sentence_one, udp_sentence_two):\n",
    "    count = 0\n",
    "    for word in sent_1:\n",
    "        if word in difference:\n",
    "            count += 1\n",
    "    for word in sent_2:\n",
    "        if word in difference:\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "#         print(sent_1, sent_2)\n",
    "        global_count += 1\n",
    "#         print(udp_sentence_one.index(sent_1))\n",
    "        ids_to_take.remove(i)\n",
    "    else:\n",
    "        udp_sentence_one_for_mod.append(sent_1)\n",
    "        udp_sentence_two_for_mod.append(sent_2)\n",
    "    i += 1\n",
    "\n",
    "print(\"Количество пар предложений, выпадающих из-за отсутствия слова в модели \", global_count)\n",
    "print(\"Общее количество пар \", len(udp_sentence_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3437"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(udp_sentence_one_for_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_udp_sentence_one = compute_tfidf(udp_sentence_one_for_mod)\n",
    "tf_idf_udp_sentence_two = compute_tfidf(udp_sentence_two_for_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "udp_vec_sentence_one = np.zeros((len(udp_sentence_one_for_mod), 300))\n",
    "udp_vec_sentence_two = np.zeros((len(udp_sentence_one_for_mod), 300))\n",
    "\n",
    "for sent_1, sent_2, tfidf_one, tfidf_two, row_id in zip(udp_sentence_one_for_mod, udp_sentence_two_for_mod, tf_idf_udp_sentence_one, tf_idf_udp_sentence_two, range(len(udp_sentence_two_for_mod))): \n",
    "    \n",
    "    udp_vec_sentence_one[row_id] = sum(tfidf_one[word] * model_w2v_rv[word] / sqrt(sum(i**2 for i in model_w2v_rv[word])) for word in sent_1)\n",
    "    udp_vec_sentence_two[row_id] = sum(tfidf_two[word] * model_w2v_rv[word] / sqrt(sum(i**2 for i in model_w2v_rv[word])) for word in sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "udp_vec_sentence_one = np.zeros((len(udp_sentence_one_for_mod), 300))\n",
    "udp_vec_sentence_two = np.zeros((len(udp_sentence_one_for_mod), 300))\n",
    "\n",
    "for sent_1, sent_2, tfidf_one, tfidf_two, row_id in zip(udp_sentence_one_for_mod, udp_sentence_two_for_mod, tf_idf_udp_sentence_one, tf_idf_udp_sentence_two, range(len(udp_sentence_two_for_mod))): \n",
    "    \n",
    "    udp_vec_sentence_one[row_id] = sum(tfidf_one[word] * model_w2v_rv[word] / sqrt(sum(i**2 for i in model_w2v_rv[word])) for word in sent_1)\n",
    "    udp_vec_sentence_two[row_id] = sum(tfidf_two[word] * model_w2v_rv[word] / sqrt(sum(i**2 for i in model_w2v_rv[word])) for word in sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3437, 600)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rv = np.hstack((udp_vec_sentence_one, udp_vec_sentence_two))\n",
    "X_rv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rv = np.array([int(i) for i in xml_df_fltr.loc[xml_df_fltr.index[ids_to_take],'class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data is  2832\n",
      "Proportion of no subscription data in oversampled data is  0.0\n",
      "Proportion of subscription data in oversampled data is  0.00035310734463276836\n"
     ]
    }
   ],
   "source": [
    "os = SMOTE(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rv, y_rv, test_size=0.3, random_state=0)\n",
    "os_data_X, os_data_y = os.fit_sample(X_train, y_train)\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_data_X))\n",
    "print(\"Proportion of no subscription data in oversampled data is \", np.sum(os_data_y) / len(os_data_X))\n",
    "print(\"Proportion of subscription data in oversampled data is \", (1 - np.sum(os_data_y)) / len(os_data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(os_data_X, os_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.65\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.65      0.68       595\n",
      "           1       0.58      0.65      0.61       437\n",
      "\n",
      "    accuracy                           0.65      1032\n",
      "   macro avg       0.65      0.65      0.65      1032\n",
      "weighted avg       0.66      0.65      0.65      1032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
